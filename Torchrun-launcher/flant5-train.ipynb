{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e93d8732-5b71-4ef7-9cec-7ab8b3e23020",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fd6088fe-b98b-4b30-ac3d-b50eb006da31",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"argolis-lsj-test\"\n",
    "REGION = \"us-central1\"\n",
    "GCS_BUCKET = \"pipeline-argolis-lsj-test-unique\"\n",
    "\n",
    "TRAIN_DOCKER_URI = f\"us-central1-docker.pkg.dev/argolis-lsj-test/t5/finetuning_flan_t5_large:multi-node-torchrun-0905\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c5b2940-f871-4bfc-bda1-ffa0c4ec07c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "The push refers to repository [us-central1-docker.pkg.dev/argolis-lsj-test/t5/finetuning_flan_t5_large]\n",
      "tag does not exist: us-central1-docker.pkg.dev/argolis-lsj-test/t5/finetuning_flan_t5_large:multi-node-torchrun-0822\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "#%cd torchrun\n",
    "!docker build -t {TRAIN_DOCKER_URI} .\n",
    "!docker push {TRAIN_DOCKER_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa5df91-8736-4d77-8d0c-49b60726c95a",
   "metadata": {},
   "source": [
    "# Cloud train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d07f3616-4005-4c3d-b0f5-6e6c991a116f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List\n",
    "import datetime\n",
    "from pytz import timezone\n",
    "import math\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "def launch_job(job_name: str,\n",
    "               project: str,\n",
    "               location: str,\n",
    "               gcs_bucket: str,\n",
    "               image_uri: str,\n",
    "               trainer_args: List[Any],\n",
    "               num_nodes: int = 1,\n",
    "               machine_type: str = \"n1-starndard-8\",\n",
    "               num_gpus_per_node: int = 1,\n",
    "               gpu_type: str = \"NVIDIA_TESLA_V100\",\n",
    "               max_bandwith_per_node: int = 100,\n",
    "               reduction_server: bool = True):\n",
    "    aiplatform.init(project=project, location=location, staging_bucket=gcs_bucket)\n",
    "    \n",
    "    # Train contaienr spec.\n",
    "    image_args = trainer_args\n",
    "    train_container_spec = {\n",
    "        \"image_uri\": image_uri,\n",
    "        \"args\": image_args,\n",
    "    }\n",
    "    \n",
    "    # Primary worker spec.\n",
    "    primary_worker_spec = {\n",
    "        \"container_spec\": train_container_spec,\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": machine_type,\n",
    "            \"accelerator_type\": gpu_type,\n",
    "            \"accelerator_count\": num_gpus_per_node,\n",
    "        },\n",
    "        \"replica_count\": 1,\n",
    "    }\n",
    "    \n",
    "    # Secondary worker spec.\n",
    "    secondary_worker_spec = {}\n",
    "    if num_nodes > 1:\n",
    "        secondary_worker_spec = {\n",
    "            \"container_spec\": train_container_spec,\n",
    "            \"machine_spec\": {\n",
    "                \"machine_type\": machine_type,\n",
    "                \"accelerator_type\": gpu_type,\n",
    "                \"accelerator_count\": num_gpus_per_node,\n",
    "            },\n",
    "            \"replica_count\": num_nodes - 1,\n",
    "        }\n",
    "    \n",
    "    # Reduction server spec.\n",
    "    # https://cloud.google.com/vertex-ai/docs/training/distributed-training#reduce_training_time_with_reduction_server\n",
    "    max_bandwith = max_bandwith_per_node * num_nodes\n",
    "    replica_count = int(math.ceil(max_bandwith / 32))\n",
    "    reduction_server_spec = {}\n",
    "    if reduction_server:\n",
    "        reduction_server_spec = {\n",
    "            \"container_spec\": {\n",
    "                \"image_uri\": \"us-docker.pkg.dev/vertex-ai-restricted/training/reductionserver:latest\",\n",
    "            },\n",
    "            \"machine_spec\": {\n",
    "                \"machine_type\": \"n1-standard-32\",\n",
    "            },\n",
    "            \"replica_count\": replica_count,\n",
    "        }\n",
    "\n",
    "    # Launch job.\n",
    "    worker_pool_specs = [\n",
    "        primary_worker_spec,\n",
    "        secondary_worker_spec,\n",
    "        reduction_server_spec,\n",
    "        {},\n",
    "    ]\n",
    "    job = aiplatform.CustomJob(\n",
    "        display_name=job_name,\n",
    "        worker_pool_specs=worker_pool_specs,\n",
    "    )\n",
    "    job.submit(\n",
    "        network=None,\n",
    "        restart_job_on_worker_restart=True,\n",
    "        enable_web_access=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6795282e-195c-4fed-96f6-9c81233deb10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating CustomJob\n",
      "CustomJob created. Resource name: projects/703099487153/locations/us-central1/customJobs/4820619134947557376\n",
      "To use this CustomJob in another session:\n",
      "custom_job = aiplatform.CustomJob.get('projects/703099487153/locations/us-central1/customJobs/4820619134947557376')\n",
      "View Custom Job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/4820619134947557376?project=703099487153\n"
     ]
    }
   ],
   "source": [
    "# Setup the cluster config.\n",
    "num_nodes = 4\n",
    "num_gpus_per_node = 2\n",
    "machine_type = \"n1-standard-32\"\n",
    "gpu_type = \"NVIDIA_TESLA_T4\"\n",
    "\n",
    "# Setup job name.\n",
    "timestamp = datetime.datetime.now().astimezone(timezone('US/Pacific')).strftime(\"%Y%m%d_%H%M%S\")\n",
    "job_name = f\"flant5-finetuning-gpu-deepspeed-torchrun\"\n",
    "\n",
    "# Setup the trainer args.\n",
    "trainer_args = [\n",
    "    f\"--nproc-per-node={num_gpus_per_node}\",  \n",
    "    \"run_seq2seq_deepspeed.py\",\n",
    "    \"--epoch=1\",\n",
    "    \"--batch_size=8\",\n",
    "    \"--train_dataset_path=/gcs/lsj-public/deepspeed/split_data_2/train\",\n",
    "    \"--test_dataset_path=/gcs/lsj-public/deepspeed/split_data_2/eval\",\n",
    "    \"--model_output_dir=$AIP_MODEL_DIR\",\n",
    "    \"--tensorboard_log_dir=$AIP_TENSORBOARD_LOG_DIR\"\n",
    "    \n",
    "]\n",
    "\n",
    "launch_job(\n",
    "    job_name=job_name,\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    gcs_bucket=GCS_BUCKET,\n",
    "    image_uri=TRAIN_DOCKER_URI,\n",
    "    trainer_args=trainer_args,\n",
    "    num_nodes=num_nodes,\n",
    "    machine_type=machine_type,\n",
    "    num_gpus_per_node=num_gpus_per_node,\n",
    "    gpu_type=gpu_type,\n",
    "    reduction_server=True,\n",
    "    #max_bandwith_per_node=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ea01e7-017e-4fa7-83ac-145a07379b9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m109",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m109"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
